{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3dad9010",
   "metadata": {},
   "source": [
    "## Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17ed88f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import time\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "587283cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/Volumes/GoogleDrive/My Drive/_My Data Analytics Exercise/Exercise/Biopython/CE/'\n",
    "frame = pd.read_csv(path+'all_interactions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac76e114",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1124 entries, 0 to 1123\n",
      "Data columns (total 5 columns):\n",
      " #   Column         Non-Null Count  Dtype \n",
      "---  ------         --------------  ----- \n",
      " 0   interactionId  1124 non-null   object\n",
      " 1   sentence       1124 non-null   object\n",
      " 2   entities       1124 non-null   object\n",
      " 3   interaction    1124 non-null   object\n",
      " 4   relationship   1124 non-null   int64 \n",
      "dtypes: int64(1), object(4)\n",
      "memory usage: 44.0+ KB\n"
     ]
    }
   ],
   "source": [
    "frame.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a3eab7d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    73.4\n",
       "0    26.6\n",
       "Name: relationship, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the positive relationship is based on the what indicated in the original data set.\n",
    "round(frame['relationship'].value_counts()/1124*100,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc383156",
   "metadata": {},
   "source": [
    "## Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "68d98333",
   "metadata": {},
   "outputs": [],
   "source": [
    "have_relationship = frame[frame['relationship']==1]\n",
    "have_no_relationship = frame[frame['relationship']==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "04cebb06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 825 entries, 1 to 1122\n",
      "Data columns (total 5 columns):\n",
      " #   Column         Non-Null Count  Dtype \n",
      "---  ------         --------------  ----- \n",
      " 0   interactionId  825 non-null    object\n",
      " 1   sentence       825 non-null    object\n",
      " 2   entities       825 non-null    object\n",
      " 3   interaction    825 non-null    object\n",
      " 4   relationship   825 non-null    int64 \n",
      "dtypes: int64(1), object(4)\n",
      "memory usage: 38.7+ KB\n"
     ]
    }
   ],
   "source": [
    "have_relationship.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f8946c81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>interactionId</th>\n",
       "      <th>sentence</th>\n",
       "      <th>entities</th>\n",
       "      <th>interaction</th>\n",
       "      <th>relationship</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BioInfer_d682</td>\n",
       "      <td>The recombinant material is similar to authent...</td>\n",
       "      <td>[('T1', 'Individual_protein', 'skeletal actin'...</td>\n",
       "      <td>{'R1': ('T1', 'T3')}</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BioInfer_d595</td>\n",
       "      <td>The common denominator is impaired beta-cateni...</td>\n",
       "      <td>[('T1', 'Individual_protein', 'beta-catenin', ...</td>\n",
       "      <td>{'R1': ('T1', 'T2'), 'R2': ('T1', 'T3')}</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>BioInfer_d538</td>\n",
       "      <td>Significantly, those actin mutants exhibiting ...</td>\n",
       "      <td>[('T4', 'Gene/protein/RNA', 'actin', (21, 26))...</td>\n",
       "      <td>{'R1': ('T1', 'T2'), 'R2': ('T1', 'T3')}</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>BioInfer_d54</td>\n",
       "      <td>Analyses of dynamic light scattering data by s...</td>\n",
       "      <td>[('T6', 'Individual_protein', 'actin', (129, 1...</td>\n",
       "      <td>{'R1': ('T1', 'T3'), 'R2': ('T2', 'T4'), 'R3':...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>BioInfer_d777</td>\n",
       "      <td>Treatment with HNE resulted in activation of e...</td>\n",
       "      <td>[('T6', 'Protein_family_or_group', 'extracellu...</td>\n",
       "      <td>{'R1': ('T1', 'T6'), 'R2': ('T2', 'T6')}</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    interactionId                                           sentence  \\\n",
       "1   BioInfer_d682  The recombinant material is similar to authent...   \n",
       "3   BioInfer_d595  The common denominator is impaired beta-cateni...   \n",
       "8   BioInfer_d538  Significantly, those actin mutants exhibiting ...   \n",
       "11   BioInfer_d54  Analyses of dynamic light scattering data by s...   \n",
       "12  BioInfer_d777  Treatment with HNE resulted in activation of e...   \n",
       "\n",
       "                                             entities  \\\n",
       "1   [('T1', 'Individual_protein', 'skeletal actin'...   \n",
       "3   [('T1', 'Individual_protein', 'beta-catenin', ...   \n",
       "8   [('T4', 'Gene/protein/RNA', 'actin', (21, 26))...   \n",
       "11  [('T6', 'Individual_protein', 'actin', (129, 1...   \n",
       "12  [('T6', 'Protein_family_or_group', 'extracellu...   \n",
       "\n",
       "                                          interaction  relationship  \n",
       "1                                {'R1': ('T1', 'T3')}             1  \n",
       "3            {'R1': ('T1', 'T2'), 'R2': ('T1', 'T3')}             1  \n",
       "8            {'R1': ('T1', 'T2'), 'R2': ('T1', 'T3')}             1  \n",
       "11  {'R1': ('T1', 'T3'), 'R2': ('T2', 'T4'), 'R3':...             1  \n",
       "12           {'R1': ('T1', 'T6'), 'R2': ('T2', 'T6')}             1  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "have_relationship.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1bc1d929",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve one of the 825 sample sentences for detailed examination\n",
    "entities = have_relationship.loc[11, 'entities']\n",
    "interaction = have_relationship.loc[11, 'interaction']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "360789c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"[('T6', 'Individual_protein', 'actin', (129, 134)), ('T4', 'Individual_protein', 'actin', (157, 162)), ('T5', 'Individual_protein', 'vinculin', (258, 266)), ('T1', 'Individual_protein', 'actin', (248, 253)), ('T3', 'Individual_protein', 'talin', (233, 238)), ('T2', 'Individual_protein', 'vinculin', (81, 89))]\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entities # in string and not ready for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dd4f24f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"{'R1': ('T1', 'T3'), 'R2': ('T2', 'T4'), 'R3': ('T2', 'T6'), 'R4': ('T3', 'T5')}\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interaction # in string and not ready for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d216a8d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('T6', 'Individual_protein', 'actin', 129, 134),\n",
       " ('T4', 'Individual_protein', 'actin', 157, 162),\n",
       " ('T5', 'Individual_protein', 'vinculin', 258, 266),\n",
       " ('T1', 'Individual_protein', 'actin', 248, 253),\n",
       " ('T3', 'Individual_protein', 'talin', 233, 238),\n",
       " ('T2', 'Individual_protein', 'vinculin', 81, 89)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Helper function to convert the entity string into individual components\n",
    "def extractEntitiesfmDF(txt):\n",
    "    identity = []\n",
    "    txt = txt.replace(\"[\",\"\").replace(\"]\",\"\").replace(\"'\",\"\").replace(\"(\",\"\")\n",
    "    entities = txt.split(\"), \")\n",
    "    for entry in entities:\n",
    "        atoms = entry.replace(\")\",'').split(\", \")\n",
    "        identity.append((atoms[0], atoms[1], atoms[2], int(atoms[3]), int(atoms[4])))\n",
    "    return identity\n",
    "e = extractEntitiesfmDF(entities)\n",
    "e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6803c784",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('R1', ('T1', 'T3')),\n",
       "             ('R2', ('T2', 'T4')),\n",
       "             ('R3', ('T2', 'T6')),\n",
       "             ('R4', ('T3', 'T5'))])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Helper function to convert the relation string into individual components\n",
    "from collections import OrderedDict\n",
    "def extractInteractionfmDF(txt):\n",
    "    txt = txt.replace(\"'\",'').replace('{','').replace('}','')\n",
    "    s = [match.start() for match in re.finditer('R', txt)]\n",
    "    s.append(len(txt)+2) # add the terminal position of the last interaction\n",
    "    length = len(s) # number of position in the position list\n",
    "    i = OrderedDict()\n",
    "    for index in range(length-1):\n",
    "        start = s[index]\n",
    "        end = s[index+1]-2\n",
    "        #print (start, end)\n",
    "        relationId, pair = txt[start:end].split(': ')\n",
    "        head, tail = pair.replace('(','').replace(')','').split(', ')\n",
    "        i[relationId] = (head, tail)\n",
    "    return i\n",
    "relations = extractInteractionfmDF(interaction)\n",
    "relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c8a44178",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyses of dynamic light scattering data by stretched exponential fit show that vinculin has a negligible influence on internal actin filament dynamics and actin bending stiffness which contrasts with our previous observations with talin , another actin and vinculin-binding protein from focal adhesions .\n"
     ]
    }
   ],
   "source": [
    "sent = have_relationship.loc[11, 'sentence'].replace(',',' ,').replace('.',' .')\n",
    "print (sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e0ea1fc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-18 14:31:42 INFO: Loading these models for language: en (English):\n",
      "==========================\n",
      "| Processor | Package    |\n",
      "--------------------------\n",
      "| tokenize  | craft      |\n",
      "| pos       | craft      |\n",
      "| lemma     | craft      |\n",
      "| depparse  | craft      |\n",
      "| ner       | bionlp13cg |\n",
      "==========================\n",
      "\n",
      "2022-01-18 14:31:42 INFO: Use device: cpu\n",
      "2022-01-18 14:31:42 INFO: Loading: tokenize\n",
      "2022-01-18 14:31:42 INFO: Loading: pos\n",
      "2022-01-18 14:31:42 INFO: Loading: lemma\n",
      "2022-01-18 14:31:43 INFO: Loading: depparse\n",
      "2022-01-18 14:31:43 INFO: Loading: ner\n",
      "2022-01-18 14:31:43 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "nlp = stanza.Pipeline('en', package='craft', processors={'ner': 'BioNLP13CG'})\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e2dea308",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entity: vinculin          \ttype: GENE_OR_GENE_PRODUCT\n",
      "entity: actin             \ttype: GENE_OR_GENE_PRODUCT\n",
      "entity: actin             \ttype: GENE_OR_GENE_PRODUCT\n",
      "entity: talin             \ttype: GENE_OR_GENE_PRODUCT\n",
      "entity: actin             \ttype: GENE_OR_GENE_PRODUCT\n",
      "entity: vinculin          \ttype: GENE_OR_GENE_PRODUCT\n",
      "entity: focal adhesions   \ttype: CELLULAR_COMPONENT\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(sent)\n",
    "print(*[f'entity: {ent.text.ljust(18, \" \")}\\ttype: {ent.type}' for sent in doc.sentences for ent in sent.ents], sep='\\n')\n",
    "# These entities correspond exactly to the original annotated items except the last entry 'focual adhesion'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "4c55c8f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id: 1\tword: Analyses  \tupos: NOUN \thead id: 11\thead: show      \tdeprel: nsubj\n",
      "id: 2\tword: of        \tupos: ADP  \thead id: 6\thead: data      \tdeprel: case\n",
      "id: 3\tword: dynamic   \tupos: ADJ  \thead id: 6\thead: data      \tdeprel: amod\n",
      "id: 4\tword: light     \tupos: NOUN \thead id: 6\thead: data      \tdeprel: compound\n",
      "id: 5\tword: scattering\tupos: NOUN \thead id: 6\thead: data      \tdeprel: compound\n",
      "id: 6\tword: data      \tupos: NOUN \thead id: 1\thead: Analyses  \tdeprel: nmod\n",
      "id: 7\tword: by        \tupos: ADP  \thead id: 10\thead: fit       \tdeprel: case\n",
      "id: 8\tword: stretched \tupos: VERB \thead id: 10\thead: fit       \tdeprel: amod\n",
      "id: 9\tword: exponential\tupos: ADJ  \thead id: 10\thead: fit       \tdeprel: amod\n",
      "id: 10\tword: fit       \tupos: NOUN \thead id: 1\thead: Analyses  \tdeprel: nmod\n",
      "id: 11\tword: show      \tupos: VERB \thead id: 0\thead: root\tdeprel: root\n",
      "id: 12\tword: that      \tupos: SCONJ\thead id: 14\thead: has       \tdeprel: mark\n",
      "id: 13\tword: vinculin  \tupos: NOUN \thead id: 14\thead: has       \tdeprel: nsubj\n",
      "id: 14\tword: has       \tupos: VERB \thead id: 11\thead: show      \tdeprel: ccomp\n",
      "id: 15\tword: a         \tupos: DET  \thead id: 17\thead: influence \tdeprel: det\n",
      "id: 16\tword: negligible\tupos: ADJ  \thead id: 17\thead: influence \tdeprel: amod\n",
      "id: 17\tword: influence \tupos: NOUN \thead id: 14\thead: has       \tdeprel: obj\n",
      "id: 18\tword: on        \tupos: ADP  \thead id: 22\thead: dynamics  \tdeprel: case\n",
      "id: 19\tword: internal  \tupos: ADJ  \thead id: 21\thead: filament  \tdeprel: amod\n",
      "id: 20\tword: actin     \tupos: NOUN \thead id: 21\thead: filament  \tdeprel: compound\n",
      "id: 21\tword: filament  \tupos: NOUN \thead id: 22\thead: dynamics  \tdeprel: compound\n",
      "id: 22\tword: dynamics  \tupos: NOUN \thead id: 17\thead: influence \tdeprel: nmod\n",
      "id: 23\tword: and       \tupos: CONJ \thead id: 26\thead: stiffness \tdeprel: cc\n",
      "id: 24\tword: actin     \tupos: NOUN \thead id: 26\thead: stiffness \tdeprel: compound\n",
      "id: 25\tword: bending   \tupos: NOUN \thead id: 26\thead: stiffness \tdeprel: compound\n",
      "id: 26\tword: stiffness \tupos: NOUN \thead id: 22\thead: dynamics  \tdeprel: conj\n",
      "id: 27\tword: which     \tupos: PRON \thead id: 28\thead: contrasts \tdeprel: nsubj\n",
      "id: 28\tword: contrasts \tupos: VERB \thead id: 17\thead: influence \tdeprel: acl:relcl\n",
      "id: 29\tword: with      \tupos: ADP  \thead id: 32\thead: observations\tdeprel: case\n",
      "id: 30\tword: our       \tupos: PRON \thead id: 32\thead: observations\tdeprel: nmod:poss\n",
      "id: 31\tword: previous  \tupos: ADJ  \thead id: 32\thead: observations\tdeprel: amod\n",
      "id: 32\tword: observations\tupos: NOUN \thead id: 28\thead: contrasts \tdeprel: obl\n",
      "id: 33\tword: with      \tupos: ADP  \thead id: 34\thead: talin     \tdeprel: case\n",
      "id: 34\tword: talin     \tupos: NOUN \thead id: 32\thead: observations\tdeprel: nmod\n",
      "id: 35\tword: ,         \tupos: PUNCT\thead id: 34\thead: talin     \tdeprel: punct\n",
      "id: 36\tword: another   \tupos: DET  \thead id: 42\thead: protein   \tdeprel: det\n",
      "id: 37\tword: actin     \tupos: NOUN \thead id: 42\thead: protein   \tdeprel: compound\n",
      "id: 38\tword: and       \tupos: CONJ \thead id: 41\thead: binding   \tdeprel: cc\n",
      "id: 39\tword: vinculin  \tupos: NOUN \thead id: 41\thead: binding   \tdeprel: dep\n",
      "id: 40\tword: -         \tupos: PUNCT\thead id: 41\thead: binding   \tdeprel: punct\n",
      "id: 41\tword: binding   \tupos: VERB \thead id: 42\thead: protein   \tdeprel: amod\n",
      "id: 42\tword: protein   \tupos: NOUN \thead id: 34\thead: talin     \tdeprel: appos\n",
      "id: 43\tword: from      \tupos: ADP  \thead id: 45\thead: adhesions \tdeprel: case\n",
      "id: 44\tword: focal     \tupos: ADJ  \thead id: 45\thead: adhesions \tdeprel: amod\n",
      "id: 45\tword: adhesions \tupos: NOUN \thead id: 42\thead: protein   \tdeprel: nmod\n",
      "id: 46\tword: .         \tupos: PUNCT\thead id: 11\thead: show      \tdeprel: punct\n"
     ]
    }
   ],
   "source": [
    "print(*[f'id: {word.id}\\tword: {word.text.ljust(10, \" \")}\\tupos: {word.upos.ljust(5, \" \")}\\thead id: {word.head}\\thead: {sent.words[word.head-1].text.ljust(10, \" \") if word.head > 0 else \"root\"}\\tdeprel: {word.deprel}' for sent in doc.sentences for word in sent.words], sep='\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e5fc7af",
   "metadata": {},
   "source": [
    "#### This shows the POS and the dependency of indiviudal word processed by default stanza nlp processor.  However, there are words that should be combined to provde better meaning, which it will facilitate the extraction of meaning that makes sense to a human reader. \n",
    "> E.g. 'dynamic light scattering data', 'stretched exponential fit', 'internal actin filament dynamics', etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9acd088",
   "metadata": {},
   "source": [
    "#### Some way to re-tokenize the sentence that makes better sense of given context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b1616ee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dynamic light\n",
      "exponential fit\n",
      "negligible influence\n",
      "internal actin\n",
      "previous observations\n",
      "focal adhesions\n"
     ]
    }
   ],
   "source": [
    "for sent in doc.sentences:\n",
    "    for i in range(len(sent.words)-1):\n",
    "        if sent.words[i].upos == 'ADJ' and sent.words[i+1].upos=='NOUN': # the one of many heuristics to recombine tokens\n",
    "            print (sent.words[i].text, sent.words[i+1].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8d645575",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions to help re-tokenize the text\n",
    "\n",
    "def firstPass(doc):\n",
    "    \"\"\" this works on doc that is tokenized by simple split statement and parsed by default stanza nlp processor\"\"\"\n",
    "    retokenized_txt = []\n",
    "    for sent in doc.sentences:\n",
    "        count = 0\n",
    "        while count < len(sent.words):\n",
    "            if sent.words[count].upos == 'ADJ' and sent.words[count+1].upos=='NOUN':\n",
    "                #print (sent.words[count].text, sent.words[count].upos, sent.words[count+1].text, sent.words[count+1].upos)\n",
    "                retokenized_txt.append(' '.join([sent.words[count].text, sent.words[count+1].text]))\n",
    "                count +=2\n",
    "            elif sent.words[count].upos == 'NOUN' and sent.words[count+1].upos=='NOUN':\n",
    "                #print (sent.words[count].text, sent.words[count].upos, sent.words[count+1].text, sent.words[count+1].upos)\n",
    "                retokenized_txt.append(' '.join([sent.words[count].text, sent.words[count+1].text]))\n",
    "                count +=2\n",
    "            else:\n",
    "                #print (sent.words[count].text)\n",
    "                retokenized_txt.append(sent.words[count].text)\n",
    "                count +=1\n",
    "    return retokenized_txt\n",
    "\n",
    "def secondPass(doc):\n",
    "    \"\"\" this works on doc that is already re-tokenized and parsed by default stanza nlp processor\"\"\"\n",
    "    retokenized_txt = []\n",
    "    for sent in doc.sentences:\n",
    "        count = 0\n",
    "        while count < len(sent.words):\n",
    "            if sent.words[count].upos == 'ADJ' and sent.words[count+1].upos == 'NOUN':\n",
    "                #print (count, sent.words[count].text, sent.words[count+1].text)\n",
    "                retokenized_txt.append(' '.join([sent.words[count].text, sent.words[count+1].text]))\n",
    "                count +=2\n",
    "            elif sent.words[count].upos == 'NOUN' and sent.words[count+1].upos == 'NOUN':\n",
    "                #print (count, sent.words[count].text, sent.words[count+1].text)\n",
    "                retokenized_txt.append(' '.join([sent.words[count].text, sent.words[count+1].text]))\n",
    "                count +=2\n",
    "            elif (sent.words[count].upos == 'VERB' and (sent.words[count].xpos == 'VBG' or sent.words[count].xpos == 'VBN')) and sent.words[count+1].upos == 'NOUN':\n",
    "                retokenized_txt.append(' '.join([sent.words[count].text, sent.words[count+1].text]))\n",
    "                #print (count, sent.words[count].text, sent.words[count+1].text)\n",
    "                count +=2\n",
    "            else:\n",
    "                retokenized_txt.append(sent.words[count].text)\n",
    "                #print (count, sent.words[count].text)\n",
    "                count +=1\n",
    "    return retokenized_txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ab989c",
   "metadata": {},
   "source": [
    "#### Full run on the original sentence through two passess that will consolidate words/phrases to a level that give better context to analyse the relationship between \n",
    "> 'talin',\n",
    "> 'actin' and \n",
    "> 'vinculin'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fdffadc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-18 15:10:25 INFO: Loading these models for language: en (English):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | craft   |\n",
      "| pos       | craft   |\n",
      "| lemma     | craft   |\n",
      "| depparse  | craft   |\n",
      "=======================\n",
      "\n",
      "2022-01-18 15:10:25 INFO: Use device: cpu\n",
      "2022-01-18 15:10:25 INFO: Loading: tokenize\n",
      "2022-01-18 15:10:25 INFO: Loading: pos\n",
      "2022-01-18 15:10:25 INFO: Loading: lemma\n",
      "2022-01-18 15:10:25 INFO: Loading: depparse\n",
      "2022-01-18 15:10:25 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id: 1\tword: Analyses          \tupos: NOUN  \thead: show                \tdeprel: nsubj\n",
      "id: 2\tword: of                \tupos: ADP   \thead: dynamic light scattering data\tdeprel: case\n",
      "id: 3\tword: dynamic light scattering data\tupos: NOUN  \thead: Analyses            \tdeprel: nmod\n",
      "id: 4\tword: by                \tupos: ADP   \thead: stretched exponential fit\tdeprel: case\n",
      "id: 5\tword: stretched exponential fit\tupos: NOUN  \thead: Analyses            \tdeprel: nmod\n",
      "id: 6\tword: show              \tupos: VERB  \thead: root\tdeprel: root\n",
      "id: 7\tword: that              \tupos: SCONJ \thead: has                 \tdeprel: mark\n",
      "id: 8\tword: vinculin          \tupos: NOUN  \thead: has                 \tdeprel: nsubj\n",
      "id: 9\tword: has               \tupos: VERB  \thead: show                \tdeprel: ccomp\n",
      "id: 10\tword: a                 \tupos: DET   \thead: negligible influence\tdeprel: det\n",
      "id: 11\tword: negligible influence\tupos: NOUN  \thead: has                 \tdeprel: obj\n",
      "id: 12\tword: on                \tupos: ADP   \thead: internal actin filament dynamics\tdeprel: case\n",
      "id: 13\tword: internal actin filament dynamics\tupos: NOUN  \thead: negligible influence\tdeprel: nmod\n",
      "id: 14\tword: and               \tupos: CONJ  \thead: actin bending stiffness\tdeprel: cc\n",
      "id: 15\tword: actin bending stiffness\tupos: NOUN  \thead: internal actin filament dynamics\tdeprel: conj\n",
      "id: 16\tword: which             \tupos: PRON  \thead: contrasts           \tdeprel: nsubj\n",
      "id: 17\tword: contrasts         \tupos: VERB  \thead: negligible influence\tdeprel: acl:relcl\n",
      "id: 18\tword: with              \tupos: ADP   \thead: previous observations\tdeprel: case\n",
      "id: 19\tword: our               \tupos: PRON  \thead: previous observations\tdeprel: nmod:poss\n",
      "id: 20\tword: previous observations\tupos: NOUN  \thead: contrasts           \tdeprel: obl\n",
      "id: 21\tword: with              \tupos: ADP   \thead: talin               \tdeprel: case\n",
      "id: 22\tword: talin             \tupos: NOUN  \thead: previous observations\tdeprel: nmod\n",
      "id: 23\tword: ,                 \tupos: PUNCT \thead: talin               \tdeprel: punct\n",
      "id: 24\tword: another           \tupos: DET   \thead: actin               \tdeprel: det\n",
      "id: 25\tword: actin             \tupos: NOUN  \thead: talin               \tdeprel: appos\n",
      "id: 26\tword: and               \tupos: CONJ  \thead: vinculin-binding protein\tdeprel: cc\n",
      "id: 27\tword: vinculin-binding protein\tupos: NOUN  \thead: talin               \tdeprel: conj\n",
      "id: 28\tword: from              \tupos: ADP   \thead: focal adhesions     \tdeprel: case\n",
      "id: 29\tword: focal adhesions   \tupos: NOUN  \thead: actin               \tdeprel: nmod\n",
      "id: 30\tword: .                 \tupos: PUNCT \thead: show                \tdeprel: punct\n",
      "CPU times: user 3.84 s, sys: 1.34 s, total: 5.18 s\n",
      "Wall time: 733 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "pretokenized_nlp = stanza.Pipeline(lang='en', package='craft', processor='tokenize', tokenize_pretokenized=True)\n",
    "\n",
    "pretokenized_sent = have_relationship.loc[11, 'sentence'].replace(',',' ,').replace('.',' .').split(' ')\n",
    "doc = pretokenized_nlp([pretokenized_sent])\n",
    "\n",
    "pretokenized_firstPass_sent = firstPass(doc)\n",
    "after_firstPass_doc = pretokenized_nlp([pretokenized_firstPass_sent])\n",
    "\n",
    "pretokenized_SecondPass_sent = secondPass(after_firstPass_doc)\n",
    "after_secondPass_doc = pretokenized_nlp([pretokenized_SecondPass_sent])\n",
    "\n",
    "print(*[f'id: {word.id}\\tword: {word.text.ljust(18, \" \")}\\tupos: {word.upos.ljust(6, \" \")}\\thead: {sent.words[word.head-1].text.ljust(20, \" \") if word.head > 0 else \"root\"}\\tdeprel: {word.deprel}' for sent in after_secondPass_doc.sentences for word in sent.words], sep='\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2ae35ddf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyses  ->  show\n",
      "of  ->  dynamic light scattering data\n",
      "dynamic light scattering data  ->  Analyses\n",
      "by  ->  stretched exponential fit\n",
      "stretched exponential fit  ->  Analyses\n",
      "show  ->  .\n",
      "that  ->  has\n",
      "vinculin  ->  has\n",
      "has  ->  show\n",
      "a  ->  negligible influence\n",
      "negligible influence  ->  has\n",
      "on  ->  internal actin filament dynamics\n",
      "internal actin filament dynamics  ->  negligible influence\n",
      "and  ->  actin bending stiffness\n",
      "actin bending stiffness  ->  internal actin filament dynamics\n",
      "which  ->  contrasts\n",
      "contrasts  ->  negligible influence\n",
      "with  ->  previous observations\n",
      "our  ->  previous observations\n",
      "previous observations  ->  contrasts\n",
      "with  ->  talin\n",
      "talin  ->  previous observations\n",
      ",  ->  talin\n",
      "another  ->  actin\n",
      "actin  ->  talin\n",
      "and  ->  vinculin-binding protein\n",
      "vinculin-binding protein  ->  talin\n",
      "from  ->  focal adhesions\n",
      "focal adhesions  ->  actin\n",
      ".  ->  show\n"
     ]
    }
   ],
   "source": [
    "# The connection between individual token based on dependency parsing result\n",
    "for sent in after_secondPass_doc.sentences:\n",
    "    for word in sent.words:\n",
    "        print (word.text, ' -> ', sent.words[word.head-1].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2a94b8",
   "metadata": {},
   "source": [
    "#### Next step \n",
    "To analyse how the three annotated entities (i.e. talin', 'actin' and 'vinculin') are related and the reason they are related"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe1e818",
   "metadata": {},
   "source": [
    "### Follow-up: Consolidate those Functions and Procedures into a Python Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33ec643f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from collections import OrderedDict\n",
    "\n",
    "def extractEntitiesfmDF(txt):\n",
    "    \"\"\" \n",
    "    txt = the string from a pandas dataframe that containes entities identified in original data set\n",
    "    \"\"\"\n",
    "    identity = []\n",
    "    txt = txt.replace(\"[\",\"\").replace(\"]\",\"\").replace(\"'\",\"\").replace(\"(\",\"\")\n",
    "    entities = txt.split(\"), \")\n",
    "    for entry in entities:\n",
    "        atoms = entry.replace(\")\",'').split(\", \")\n",
    "        identity.append((atoms[0], atoms[1], atoms[2], int(atoms[3]), int(atoms[4])))\n",
    "    return identity\n",
    "\n",
    "def extractInteractionfmDF(txt):\n",
    "    \"\"\" \n",
    "    txt = the string from a pandas dataframe that containes relations(s) identified in original data set\n",
    "    \"\"\"\n",
    "    txt = txt.replace(\"'\",'').replace('{','').replace('}','')\n",
    "    s = [match.start() for match in re.finditer('R', txt)]\n",
    "    s.append(len(txt)+2) # add the terminal position of the last interaction\n",
    "    length = len(s) # number of position in the position list\n",
    "    interaction_dict = OrderedDict()\n",
    "    for index in range(length-1):\n",
    "        start = s[index]\n",
    "        end = s[index+1]-2\n",
    "        #print (start, end)\n",
    "        relationId, pair = txt[start:end].split(': ')\n",
    "        head, tail = pair.replace('(','').replace(')','').split(', ')\n",
    "        interaction_dict[relationId] = (head, tail)\n",
    "    return interaction_dict\n",
    "\n",
    "def firstPass(doc):\n",
    "    \"\"\" this works on doc that is tokenized by simple split statement and parsed by default stanza nlp processor\"\"\"\n",
    "    retokenized_txt = []\n",
    "    for sent in doc.sentences:\n",
    "        count = 0\n",
    "        while count < len(sent.words):\n",
    "            if sent.words[count].upos == 'ADJ' and sent.words[count+1].upos=='NOUN':\n",
    "                #print (sent.words[count].text, sent.words[count].upos, sent.words[count+1].text, sent.words[count+1].upos)\n",
    "                retokenized_txt.append(' '.join([sent.words[count].text, sent.words[count+1].text]))\n",
    "                count +=2\n",
    "            elif sent.words[count].upos == 'NOUN' and sent.words[count+1].upos=='NOUN':\n",
    "                #print (sent.words[count].text, sent.words[count].upos, sent.words[count+1].text, sent.words[count+1].upos)\n",
    "                retokenized_txt.append(' '.join([sent.words[count].text, sent.words[count+1].text]))\n",
    "                count +=2\n",
    "            else:\n",
    "                #print (sent.words[count].text)\n",
    "                retokenized_txt.append(sent.words[count].text)\n",
    "                count +=1\n",
    "    return retokenized_txt\n",
    "\n",
    "def secondPass(doc):\n",
    "    \"\"\" this works on doc that is already re-tokenized and parsed by default stanza nlp processor\"\"\"\n",
    "    retokenized_txt = []\n",
    "    for sent in doc.sentences:\n",
    "        count = 0\n",
    "        while count < len(sent.words):\n",
    "            if sent.words[count].upos == 'ADJ' and sent.words[count+1].upos == 'NOUN':\n",
    "                #print (count, sent.words[count].text, sent.words[count+1].text)\n",
    "                retokenized_txt.append(' '.join([sent.words[count].text, sent.words[count+1].text]))\n",
    "                count +=2\n",
    "            elif sent.words[count].upos == 'NOUN' and sent.words[count+1].upos == 'NOUN':\n",
    "                #print (count, sent.words[count].text, sent.words[count+1].text)\n",
    "                retokenized_txt.append(' '.join([sent.words[count].text, sent.words[count+1].text]))\n",
    "                count +=2\n",
    "            elif (sent.words[count].upos == 'VERB' and (sent.words[count].xpos == 'VBG' or sent.words[count].xpos == 'VBN')) and sent.words[count+1].upos == 'NOUN':\n",
    "                retokenized_txt.append(' '.join([sent.words[count].text, sent.words[count+1].text]))\n",
    "                #print (count, sent.words[count].text, sent.words[count+1].text)\n",
    "                count +=2\n",
    "            else:\n",
    "                retokenized_txt.append(sent.words[count].text)\n",
    "                #print (count, sent.words[count].text)\n",
    "                count +=1\n",
    "    return retokenized_txt\n",
    "\n",
    "def tokenizer(nlp, text):\n",
    "    pretokenized_sent = text.replace(',',' ,').replace('.',' .').split(' ')\n",
    "    doc = nlp([pretokenized_sent])\n",
    "    pretokenized_firstPass_sent = firstPass(doc)\n",
    "    after_firstPass_doc = nlp([pretokenized_firstPass_sent])\n",
    "    pretokenized_SecondPass_sent = secondPass(after_firstPass_doc)\n",
    "    after_secondPass_doc = nlp([pretokenized_SecondPass_sent])\n",
    "    return after_secondPass_doc\n",
    "\n",
    "def plot_network(graph):\n",
    "    plt.figure(figsize=(20,20))\n",
    "    pos = nx.spring_layout(graph)\n",
    "    nx.draw_networkx_nodes(graph, pos, node_color='r')\n",
    "    nx.draw_networkx_labels(graph, pos, font_size = 14)\n",
    "    nx.draw_networkx_edges(graph, pos, edge_color='b')\n",
    "    plt.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c91174ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
